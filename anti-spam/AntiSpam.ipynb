{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Описание **\n",
    "Построить графики распределения в спам и не спам множествах следующих признаков:\n",
    "\n",
    "1\tКоличество слов на странице\n",
    "\n",
    "2\tСредняя длинна слова\n",
    "\n",
    "3\tКоличество слов в заголовке страниц (слова в теге <html><head><title> Some text </title>)\n",
    "\n",
    "4\tКоличество слов в анкорах ссылок (<html><body><a> Some text </a>)\n",
    "\n",
    "5\tКоэффициент сжатия\n",
    "\n",
    "Нужно посчитать статистику минимум по трем признакам и обязательно сделать для 1-го и 2-го признаков\n",
    "\n",
    "И отправить первое решение в соревнование https://kaggle.com/join/antispam_infopoisk\n",
    "На основании одного из указанных выше признаков попытаться разделить мн-во, так чтобы score в соревновании был больше 0.55\n",
    "\n",
    "При выполнении всех этих условия в течении семинара +1 балл к ДЗ\n",
    "\n",
    "Описание ДЗ и правил выставления за него баллов в https://inclass.kaggle.com/c/antispam-infopoisk  \n",
    "Срок для ИТМО - 1 неделя  \n",
    "Срок для Техносферы - 3 недели  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import zlib\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "ParsingResult = namedtuple('ParsingResult', ['title_text', 'links_text', 'text'])\n",
    "def parse_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    title = \"\" if soup.title is None or soup.title.string is None else soup.title.string\n",
    "    links = [link.string for link in soup.findAll('a') if link.string is not None]\n",
    "    [s.extract() for s in soup(['script', 'style'])]\n",
    "    full_text = soup.get_text()\n",
    "    return ParsingResult(title, links, full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def easy_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    word = \"\"\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = \"\"\n",
    "    if word: yield word\n",
    "        \n",
    "def text2words(text, tokenizer=easy_tokenizer):\n",
    "    return tokenizer(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WordsFeaturesResult = namedtuple('WordsFeaturesResult', ['title_text', 'links_text', 'text_words'])\n",
    "def get_words_features(url, html_data):\n",
    "    pr = parse_html(html_data)\n",
    "    return WordsFeaturesResult(pr.title_text, pr.links_text, list(text2words(pr.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "\n",
    "def load_raw_csv(input_file_name, calc_features_f):    \n",
    "    with open(input_file_name)  as input_file:\n",
    "        headers = input_file.readline()\n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            features = calc_features_f(url, html_data)            \n",
    "            yield DocItem(url_id, mark, url, features)            \n",
    "                \n",
    "        trace(i, 1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raw_csv2text_csv(name):\n",
    "    raw_csv_file = '/home/kirill/ssd-pool/kaggle_{}_data_tab.csv'.format(name)\n",
    "    with open('text_features_{}.csv'.format(name) , 'w') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow(['doc_id','is_spam', 'url', 'title_text', 'links_text', 'text_words'])\n",
    "        for doc in load_raw_csv(raw_csv_file, get_words_features):\n",
    "            writer.writerow([doc.doc_id, doc.is_spam, doc.url, \n",
    "                             doc.features.title_text, doc.features.links_text, doc.features.text_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw_csv2text_csv('train')\n",
    "#raw_csv2text_csv('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_text_to_list(raw):\n",
    "    rv = [i[1:-1].strip() for i in raw[1:-1].split(\", \")]\n",
    "    return rv\n",
    "\n",
    "# change input df\n",
    "def extract_features_from_texts(texts):\n",
    "    texts.links_text = texts.links_text.apply(split_text_to_list)\n",
    "    texts.text_words = texts.text_words.apply(split_text_to_list)\n",
    "    texts['words_num'] = texts.text_words.apply(lambda x: len(x))\n",
    "    texts['avg_word_len'] = texts.apply(lambda row: sum([len(w) for w in row['text_words']])/float(row['words_num']), \n",
    "                                        axis=1)\n",
    "    texts['title_words_num'] = texts.title_text.apply(lambda x: len(list(text2words(str(x)))))\n",
    "    texts['anchor_words_num'] = texts.links_text.apply(lambda x: sum([len(i) for i in x]))\n",
    "    texts['compression'] = texts.apply(lambda row: row['words_num'] / float(len(set(row['text_words']))), \n",
    "                                       axis=1)\n",
    "    texts['links_num'] = texts.links_text.apply(lambda x: len(x))\n",
    "    texts['domain_length'] = texts.url.apply(lambda x: len(x.split('//')[1].split('/')[0]))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts = extract_features_from_texts(pd.read_csv(\"text_features_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>is_spam</th>\n",
       "      <th>url</th>\n",
       "      <th>title_text</th>\n",
       "      <th>links_text</th>\n",
       "      <th>text_words</th>\n",
       "      <th>words_num</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>title_words_num</th>\n",
       "      <th>anchor_words_num</th>\n",
       "      <th>compression</th>\n",
       "      <th>links_num</th>\n",
       "      <th>domain_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9222401963271173253</td>\n",
       "      <td>False</td>\n",
       "      <td>http://lawleader.ru/docs/32/</td>\n",
       "      <td>Договор займа, договоры, договора</td>\n",
       "      <td>[Главная, О компании, Услуги, Бизнес-договоры,...</td>\n",
       "      <td>[договор, займа, договоры, договора, главная, ...</td>\n",
       "      <td>2914</td>\n",
       "      <td>6.580645</td>\n",
       "      <td>4</td>\n",
       "      <td>1303</td>\n",
       "      <td>2.63472</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                doc_id is_spam                           url  \\\n",
       "0 -9222401963271173253   False  http://lawleader.ru/docs/32/   \n",
       "\n",
       "                          title_text  \\\n",
       "0  Договор займа, договоры, договора   \n",
       "\n",
       "                                          links_text  \\\n",
       "0  [Главная, О компании, Услуги, Бизнес-договоры,...   \n",
       "\n",
       "                                          text_words  words_num  avg_word_len  \\\n",
       "0  [договор, займа, договоры, договора, главная, ...       2914      6.580645   \n",
       "\n",
       "   title_words_num  anchor_words_num  compression  links_num  domain_length  \n",
       "0                4              1303      2.63472         51             12  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution(feature, bins = range(0,3000,10)):\n",
    "    spam_data = train_texts[train_texts['is_spam']][feature]\n",
    "    not_spam_data = train_texts[train_texts['is_spam'] == False][feature]\n",
    "    plt.hist(spam_data, bins=bins, color='red', linewidth=0, normed=True, alpha=0.7, label='spam')\n",
    "    plt.hist(not_spam_data, bins=bins, color='blue', linewidth=0, normed=True, alpha=0.7, label='not_spam')\n",
    "    plt.title(feature)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_distribution('words_num')\n",
    "# plot_distribution('title_words_num', bins = range(0, 30, 1))\n",
    "# plot_distribution('avg_word_len', bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirill/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = KFold(train_texts.shape[0], n_folds=5, shuffle=True)\n",
    "Y = train_texts['is_spam'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_words2_strs(words_lists):\n",
    "    return [\" \".join([w for w in i if len(w) > 2]) for i in list(words_lists)]\n",
    "X_text_str = text_words2_strs(train_texts.text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to free memory\n",
    "train_texts.text_words.apply(lambda x: \"\")\n",
    "del train_texts['text_words']\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   31.0s remaining:   46.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   45.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8833642446256309"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', MultinomialNB())\n",
    "                       ])\n",
    "nb_text_pred = cross_val_predict(nb_text_clf, X_text_str, Y, folds, n_jobs=-1, verbose=True)\n",
    "nb_score = f1_score(Y, nb_text_pred, average='weighted')\n",
    "nb_score\n",
    "# CountVectorizer: 0.88261277536786598\n",
    "# CountVectorizer, TfidfTransformer: 0.8833642446256309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', \n",
    "                                           penalty='l2',alpha=1e-4, n_iter=5))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   18.0s remaining:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   18.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95552005331114609"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_text_pred = cross_val_predict(svm_text_clf, X_text_str, Y, folds, n_jobs=-1, verbose=True)\n",
    "svm_score = f1_score(Y, svm_text_pred, average='weighted')\n",
    "svm_score\n",
    "# 0.95552005331114609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...   penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_text_clf.fit(X_text_str, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts['svm_pred'] = svm_text_clf.predict(X_text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89822281114989433"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['words_num', 'avg_word_len', 'title_words_num', 'anchor_words_num', \n",
    "            'compression', 'links_num', 'domain_length']#, 'svm_pred']\n",
    "X = train_texts[features].as_matrix()\n",
    "xgb_clf = xgb.XGBClassifier(max_depth = 7, n_estimators=300, silent=False)\n",
    "xgb_pred = cross_val_predict(xgb_clf, X, Y, folds)\n",
    "score = f1_score(Y, xgb_pred, average='weighted')\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nb_text_clf.fit(X_text_str, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk: #0\n",
      "chunk: #5\n",
      "chunk: #10\n",
      "chunk: #15\n",
      "chunk: #20\n",
      "chunk: #25\n",
      "chunk: #30\n"
     ]
    }
   ],
   "source": [
    "#process test file line by line\n",
    "reader = pd.read_csv('text_features_test.csv', chunksize=500)\n",
    "chunk_predictions = []\n",
    "chunk_ids = []\n",
    "for i, chunk in enumerate(reader):\n",
    "    if i % 5 == 0:\n",
    "        print(\"chunk: #{}\".format(i))\n",
    "    data = extract_features_from_texts(chunk)\n",
    "    chunk_ids.append(data['doc_id'])\n",
    "    # svm_pred = svm_text_clf.predict(text_words2_strs(data.text_words))\n",
    "    # X = data[features].as_matrix()\n",
    "    # chunk_predictions.append(xgb_clf.predict(X))\n",
    "    X = text_words2_strs(data.text_words)\n",
    "    chunk_predictions.append(svm_text_clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16039)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(inp):\n",
    "    return sum([list(i) for i in inp], [])\n",
    "predictions = flatten(chunk_predictions)\n",
    "ids = flatten(chunk_ids)\n",
    "len(predictions), len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'Id': ids, 'Prediction': [int(i) for i in predictions]})\n",
    "result.to_csv('my_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
